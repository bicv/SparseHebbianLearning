{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Hebbian Learning: basics of coding\n",
    "\n",
    "We are interested here in learning the \"optimal\" components of a set of images (let's say some \"natural\", usual images). As there is no supervisor to guide the learning, this is called unsupervised learning. Our basic hypothesis to find the best (\"optimal\") components will be to assume that *a priori* the most sparse is more plausible. We will implement the derived algorithm in this set of scripts.\n",
    "\n",
    "Here, we will show the basic operations that are implemented in this package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T11:04:23.249365Z",
     "start_time": "2018-09-26T11:04:23.231107Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T11:04:23.559030Z",
     "start_time": "2018-09-26T11:04:23.251557Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "\n",
    "To test and control for the role of different parameters, we will have a first object (in the [shl_experiments.py](https://github.com/bicv/SparseHebbianLearning/blob/master/shl_scripts/shl_experiments.py) script) that controls a learning experiment. It contains all relevant parameters, but can also keep a trace of the history of some statistics. This is useful to compare the relative efficiency of the different solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T11:04:23.581050Z",
     "start_time": "2018-09-26T11:04:23.561502Z"
    }
   },
   "outputs": [],
   "source": [
    "tag = 'coding'\n",
    "homeo_methods = ['None', 'HEH']\n",
    "homeo_methods = ['None', 'HAP']\n",
    "homeo_methods = ['None', 'HAP', 'HEH']\n",
    "\n",
    "record_num_batches = 2**10\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T11:04:39.043171Z",
     "start_time": "2018-09-26T11:04:23.583309Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shl_scripts.shl_experiments import SHL\n",
    "shl = SHL()\n",
    "data = shl.get_data(matname=tag + '_test')\n",
    "indx = np.random.permutation(data.shape[0])[:record_num_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T11:04:39.189245Z",
     "start_time": "2018-09-26T11:04:39.045690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 lolo  staff  134185088 Sep 26 13:04 cache_dir/coding_test_data.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {shl.cache_dir}/{tag}*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm {shl.cache_dir}/{tag}*\n",
    "!ls -l {shl.cache_dir}/{tag}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T11:04:39.350443Z",
     "start_time": "2018-09-26T11:04:39.192483Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patches, size of patches =  (65520, 256)\n",
      "average of patches =  -4.193836974888984e-19  +/-  0.01106396040570928\n",
      "average energy of data =  0.3029360318834556 +/- 0.091074993196838\n"
     ]
    }
   ],
   "source": [
    "print('number of patches, size of patches = ', data.shape)\n",
    "print('average of patches = ', data.mean(), ' +/- ', data.mean(axis=1).std())\n",
    "SE = np.sqrt(np.mean(data**2, axis=1))\n",
    "print('average energy of data = ', SE.mean(), '+/-', SE.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning\n",
    "\n",
    "The actual learning is done in a second object (here ``dico``) from which we can access another set of properties and functions  (see the [shl_learn.py](https://github.com/bicv/SparseHebbianLearning/blob/master/shl_scripts/shl_learn.py) script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-26T11:04:23.239Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶\n",
      "ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶NoneğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶\n",
      "ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶ğŸ¶\n"
     ]
    }
   ],
   "source": [
    "list_figures = ['show_dico', 'time_plot_error', 'time_plot_logL', 'time_plot_MC', 'show_Pcum']\n",
    "\n",
    "dico = {}\n",
    "for homeo_method in homeo_methods:\n",
    "    print(42*'ğŸ¶')\n",
    "    print(19*'ğŸ¶' + homeo_method + 19*'ğŸ¶')\n",
    "    print(42*'ğŸ¶')\n",
    "    shl = SHL(homeo_method=homeo_method)\n",
    "    dico[homeo_method] = shl.learn_dico(data=data, list_figures=list_figures, matname=tag + '_' + homeo_method)\n",
    "\n",
    "    print('size of dictionary = (number of filters, size of imagelets) = ', dico[homeo_method].dictionary.shape)\n",
    "    print('average of filters = ',  dico[homeo_method].dictionary.mean(axis=1).mean(), \n",
    "          '+/-',  dico[homeo_method].dictionary.mean(axis=1).std())\n",
    "    SE = np.sqrt(np.sum(dico[homeo_method].dictionary**2, axis=1))\n",
    "    print('average energy of filters = ', SE.mean(), '+/-', SE.std())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coding\n",
    "\n",
    "The learning itself is done via a gradient descent but is highly dependent on the coding / decoding algorithm. This belongs to a another function (in the [shl_encode.py](https://github.com/bicv/SparseHebbianLearning/blob/master/shl_scripts/shl_encode.py) script)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm /tmp/cache_dir/quantization_coding.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-26T11:04:23.255Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1973)\n",
    "#sparse_code = shl.code(data, dico_, matname=matname)\n",
    "P_cum_zeroeffect = np.linspace(0, 1, shl.nb_quant, endpoint=True)[np.newaxis, :] * np.ones((shl.n_dictionary, 1))\n",
    "#P_cum = dico_.P_cum\n",
    "#P_cum = None\n",
    "#l0_sparseness = 150\n",
    "l0_sparseness = shl.l0_sparseness\n",
    "\n",
    "\n",
    "from shl_scripts.shl_tools import show_data, print_stats\n",
    "coding_gain = {}\n",
    "for homeo_method in homeo_methods:\n",
    "    coding_gain[homeo_method] = {}\n",
    "    print(33*'ğŸ¶')\n",
    "    print(15*'ğŸ¶' + homeo_method[:3] + 15*'ğŸ¶')\n",
    "    print(33*'ğŸ¶')\n",
    "\n",
    "    from shl_scripts.shl_encode import mp\n",
    "    labels = ['Non', 'One', 'Hom']\n",
    "    for P_cum, gain, label in zip([None, P_cum_zeroeffect, dico[homeo_method].P_cum], [np.ones(shl.n_dictionary), None, None], labels):\n",
    "        sparse_code = mp(data[indx, :], dico[homeo_method].dictionary, precision=dico[homeo_method].precision, l0_sparseness=l0_sparseness, P_cum=P_cum, gain=gain)\n",
    "        SD, SE = print_stats(data[indx, :], dico[homeo_method].dictionary, sparse_code)\n",
    "        coding_gain[homeo_method][label] = (SD/SE).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-26T11:04:23.258Z"
    }
   },
   "outputs": [],
   "source": [
    "coding_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-26T11:04:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information shl_scripts, numpy, shl_scripts, pandas, matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
